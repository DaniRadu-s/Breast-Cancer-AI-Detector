{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318e4d0789e6a098"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T14:43:44.978208100Z",
     "start_time": "2025-05-31T14:35:34.910482800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\preprocessor_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\model.safetensors\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15836\\2286037791.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "***** Running training *****\n",
      "  Num examples = 299\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 95\n",
      "  Number of trainable parameters = 85,800,963\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/95 : < :, Epoch 0.05/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Jurnal de antrenare:\n"
     ]
    },
    {
     "data": {
      "text/plain": "      loss  grad_norm  learning_rate     epoch  step  train_runtime  \\\n0   1.0865   1.331453   5.000000e-05  0.052632     1            NaN   \n1   1.1068   1.272911   4.947368e-05  0.105263     2            NaN   \n2   1.1142   1.497016   4.894737e-05  0.157895     3            NaN   \n3   1.0825   1.315557   4.842105e-05  0.210526     4            NaN   \n4   1.1337   1.422301   4.789474e-05  0.263158     5            NaN   \n..     ...        ...            ...       ...   ...            ...   \n91  0.9599   1.608701   2.105263e-06  4.842105    92            NaN   \n92  0.8695   1.553878   1.578947e-06  4.894737    93            NaN   \n93  0.9162   1.655388   1.052632e-06  4.947368    94            NaN   \n94  1.0164   2.432333   5.263158e-07  5.000000    95            NaN   \n95     NaN        NaN            NaN  5.000000    95       472.7125   \n\n    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n0                        NaN                     NaN           NaN         NaN  \n1                        NaN                     NaN           NaN         NaN  \n2                        NaN                     NaN           NaN         NaN  \n3                        NaN                     NaN           NaN         NaN  \n4                        NaN                     NaN           NaN         NaN  \n..                       ...                     ...           ...         ...  \n91                       NaN                     NaN           NaN         NaN  \n92                       NaN                     NaN           NaN         NaN  \n93                       NaN                     NaN           NaN         NaN  \n94                       NaN                     NaN           NaN         NaN  \n95                     3.163                   0.201  1.158516e+17    1.043211  \n\n[96 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>grad_norm</th>\n      <th>learning_rate</th>\n      <th>epoch</th>\n      <th>step</th>\n      <th>train_runtime</th>\n      <th>train_samples_per_second</th>\n      <th>train_steps_per_second</th>\n      <th>total_flos</th>\n      <th>train_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0865</td>\n      <td>1.331453</td>\n      <td>5.000000e-05</td>\n      <td>0.052632</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.1068</td>\n      <td>1.272911</td>\n      <td>4.947368e-05</td>\n      <td>0.105263</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.1142</td>\n      <td>1.497016</td>\n      <td>4.894737e-05</td>\n      <td>0.157895</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0825</td>\n      <td>1.315557</td>\n      <td>4.842105e-05</td>\n      <td>0.210526</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.1337</td>\n      <td>1.422301</td>\n      <td>4.789474e-05</td>\n      <td>0.263158</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>0.9599</td>\n      <td>1.608701</td>\n      <td>2.105263e-06</td>\n      <td>4.842105</td>\n      <td>92</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>0.8695</td>\n      <td>1.553878</td>\n      <td>1.578947e-06</td>\n      <td>4.894737</td>\n      <td>93</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>0.9162</td>\n      <td>1.655388</td>\n      <td>1.052632e-06</td>\n      <td>4.947368</td>\n      <td>94</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>1.0164</td>\n      <td>2.432333</td>\n      <td>5.263158e-07</td>\n      <td>5.000000</td>\n      <td>95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.000000</td>\n      <td>95</td>\n      <td>472.7125</td>\n      <td>3.163</td>\n      <td>0.201</td>\n      <td>1.158516e+17</td>\n      <td>1.043211</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows √ó 10 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logurile au fost salvate √Æn logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./vit-model\\model.safetensors\n",
      "Image processor saved in ./vit-model\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model salvat √Æn './vit-model'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity_info()  # Vezi loguri »ôi √Æn consolƒÉ\n",
    "\n",
    "# === CONFIGURARE ===\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "data_dir = \"data\"  # aici ai \"train\", \"val\"\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "num_classes = 3\n",
    "epochs = 5\n",
    "\n",
    "# === LOAD MODEL & PREPROCESSOR ===\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# === TRANSFORMARE ===\n",
    "transform = Compose([\n",
    "    Resize((image_size, image_size)),\n",
    "    CenterCrop(image_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "# === LOAD IMAGES ===\n",
    "def load_split(split):\n",
    "    folder = os.path.join(data_dir, split)\n",
    "    dataset = ImageFolder(folder, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "train_ds = load_split(\"train\")\n",
    "val_ds = load_split(\"val\")\n",
    "\n",
    "# === WRAP TORCH DATASET IN HF DATASET ===\n",
    "def convert_to_hf_dataset(torch_ds):\n",
    "    images, labels = [], []\n",
    "    for img, label in torch_ds:\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    return Dataset.from_dict({\"pixel_values\": images, \"labels\": labels})\n",
    "\n",
    "train_hf = convert_to_hf_dataset(train_ds)\n",
    "val_hf = convert_to_hf_dataset(val_ds)\n",
    "\n",
    "# === DEFINE MODEL ===\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# === METRICƒÇ ===\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"accuracy\": np.mean(preds == p.label_ids)}\n",
    "\n",
    "# === ARGUMENTE TRAINING ===\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./vit-output\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,  # Logare la fiecare pas\n",
    "    report_to=\"none\",  # evitƒÉ integrarea cu wandb sau tensorboard implicit\n",
    "    disable_tqdm=False,  # permite progres vizibil\n",
    "    save_strategy=\"no\",  # nu salva checkpointuri inutile\n",
    ")\n",
    "\n",
    "# === TRAINER ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_hf,\n",
    "    eval_dataset=val_hf,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# === TRAIN ===\n",
    "trainer.train()\n",
    "\n",
    "# === LOGURI ===\n",
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "print(\"\\nüìä Jurnal de antrenare:\")\n",
    "display(log_df)\n",
    "\n",
    "# === OPTIONAL: Salvare loguri\n",
    "log_df.to_csv(\"logs.csv\", index=False)\n",
    "print(\"‚úÖ Logurile au fost salvate √Æn logs.csv\")\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "model.save_pretrained(\"./vit-model\")\n",
    "processor.save_pretrained(\"./vit-model\")\n",
    "print(\"‚úÖ Model salvat √Æn './vit-model'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imbunatatire model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4246d789c7565368"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edd29a26c6434d9eb153d251912ec2e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity_info()  # ActivƒÉm loguri informative\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:04:41.631543500Z",
     "start_time": "2025-06-01T12:04:26.842648200Z"
    }
   },
   "id": "93fb0229014e71af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# configurari"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "370060aed4a979f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# === CONFIGURARE ===\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "data_dir = \"data\"  # trebuie sƒÉ con»õinƒÉ subfolderele \"train\" »ôi \"val\"\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "num_classes = 3  # normal, benign, malignant\n",
    "epochs = 5\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:04:41.641757Z",
     "start_time": "2025-06-01T12:04:41.634427300Z"
    }
   },
   "id": "68c483f228963b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Incarcare procesor si definirea transformarilor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58625666040c0cc9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lab1\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\preprocessor_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.40.1\"\n",
      "}\n",
      "\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "# === √éNCƒÇRCARE PROCESSOR ===\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# === TRANSFORMARE IMAGINI ===\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ColorJitter\n",
    "\n",
    "train_transform = Compose([\n",
    "    Resize((image_size, image_size)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomRotation(degrees=15),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize((image_size, image_size)),\n",
    "    CenterCrop(image_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:04:42.154610800Z",
     "start_time": "2025-06-01T12:04:41.640105300Z"
    }
   },
   "id": "9b1bdd1a673bfe07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Incarcare Imagini"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "309a80e59f45ea87"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "\n",
    "# === √éNCƒÇRCARE IMAGINI ===\n",
    "def load_split(split):\n",
    "    folder = os.path.join(data_dir, split)\n",
    "    if split == \"train\":\n",
    "        dataset = ImageFolder(folder, transform=train_transform)\n",
    "    else:\n",
    "        dataset = ImageFolder(folder, transform=val_transform)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = load_split(\"train\")\n",
    "val_ds = load_split(\"val\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:04:42.161378200Z",
     "start_time": "2025-06-01T12:04:42.155744400Z"
    }
   },
   "id": "bbdb642f6deb176f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conversie la HuggingFace DataSet  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79eb9ab91fa10436"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# === CONVERSIE LA HUGGINGFACE DATASET ===\n",
    "def convert_to_hf_dataset(torch_ds):\n",
    "    images, labels = [], []\n",
    "    for img, label in torch_ds:\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    return Dataset.from_dict({\"pixel_values\": images, \"labels\": labels})\n",
    "\n",
    "train_hf = convert_to_hf_dataset(train_ds)\n",
    "val_hf = convert_to_hf_dataset(val_ds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:05:01.240591500Z",
     "start_time": "2025-06-01T12:04:42.166653700Z"
    }
   },
   "id": "cc5e6a90080cf3ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definirea modelului VIT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df2ddb998a936ed"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.40.1\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\b4569560a39a0f1af58e3ddaf17facf20ab919b0\\model.safetensors\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# === DEFINIREA MODELULUI ===\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes  # Clasificare √Æn 3 categorii: normal, benign, malign\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:05:02.268159100Z",
     "start_time": "2025-06-01T12:05:01.244943700Z"
    }
   },
   "id": "7f7838f2e2851414"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definirea metricii de evaluare si setarea hyperparametrilor pentru antrenare"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d76bfa7dc09a0d7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === METRICƒÇ CUSTOM ===\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    \n",
    "    # Optional: matrice de confuzie\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    print(\"\\nüîç Matrice de confuzie:\\n\", cm)\n",
    "    print( {\"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1})\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:11:07.510644300Z",
     "start_time": "2025-06-01T13:11:07.499503200Z"
    }
   },
   "id": "7bfd8c20fc043d5e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./vit-output-improved\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:11:08.508473400Z",
     "start_time": "2025-06-01T13:11:08.441599400Z"
    }
   },
   "id": "a3938593e3bdb9bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definirea obiectului Trainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57526e1311af1608"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_hf,\n",
    "    eval_dataset=val_hf,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:11:10.479098800Z",
     "start_time": "2025-06-01T13:11:10.103483800Z"
    }
   },
   "id": "17f2eee153f45811"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Antrenarea modelului"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57eedd3cf90c8e97"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 299\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 190\n",
      "  Number of trainable parameters = 85,800,963\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/190 : < :, Epoch 0.05/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Matrice de confuzie:\n",
      " [[15  4  6]\n",
      " [ 9  1 15]\n",
      " [ 9  5 12]]\n",
      "{'accuracy': 0.3684210526315789, 'precision': 0.3068181818181818, 'recall': 0.3684210526315789, 'f1': 0.32810364602329906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./vit-output-improved\\checkpoint-19\n",
      "Configuration saved in ./vit-output-improved\\checkpoint-19\\config.json\n",
      "Model weights saved in ./vit-output-improved\\checkpoint-19\\model.safetensors\n",
      "Image processor saved in ./vit-output-improved\\checkpoint-19\\preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Matrice de confuzie:\n",
      " [[16  6  3]\n",
      " [10  2 13]\n",
      " [18  1  7]]\n",
      "{'accuracy': 0.32894736842105265, 'precision': 0.2968356332200726, 'recall': 0.32894736842105265, 'f1': 0.2889993526014525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./vit-output-improved\\checkpoint-38\n",
      "Configuration saved in ./vit-output-improved\\checkpoint-38\\config.json\n",
      "Model weights saved in ./vit-output-improved\\checkpoint-38\\model.safetensors\n",
      "Image processor saved in ./vit-output-improved\\checkpoint-38\\preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 76\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Matrice de confuzie:\n",
      " [[15  7  3]\n",
      " [11  2 12]\n",
      " [15  5  6]]\n",
      "{'accuracy': 0.3026315789473684, 'precision': 0.2650834403080873, 'recall': 0.3026315789473684, 'f1': 0.2706057473694652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./vit-output-improved\\checkpoint-57\n",
      "Configuration saved in ./vit-output-improved\\checkpoint-57\\config.json\n",
      "Model weights saved in ./vit-output-improved\\checkpoint-57\\model.safetensors\n",
      "Image processor saved in ./vit-output-improved\\checkpoint-57\\preprocessor_config.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m log_df = pd.DataFrame(trainer.state.log_history)\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33müìä Jurnal de antrenare:\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\transformers\\trainer.py:1859\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   1857\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1859\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1860\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1861\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1862\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1863\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1864\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\transformers\\trainer.py:2203\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2200\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_step_begin(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n\u001B[32m   2202\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.accumulate(model):\n\u001B[32m-> \u001B[39m\u001B[32m2203\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2205\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2206\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2207\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2208\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2209\u001B[39m ):\n\u001B[32m   2210\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2211\u001B[39m     tr_loss += tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\transformers\\trainer.py:3147\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs)\u001B[39m\n\u001B[32m   3145\u001B[39m         scaled_loss.backward()\n\u001B[32m   3146\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3147\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3149\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach() / \u001B[38;5;28mself\u001B[39m.args.gradient_accumulation_steps\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\accelerate\\accelerator.py:2473\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2471\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2472\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2473\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Lab1\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "print(\"\\nüìä Jurnal de antrenare:\")\n",
    "display(log_df)\n",
    "\n",
    "# Salvare √Æn CSV\n",
    "log_df.to_csv(\"logs.csv\", index=False)\n",
    "print(\"‚úÖ Logurile au fost salvate √Æn logs.csv\")\n",
    "model.save_pretrained(\"./vit-model-improved\")\n",
    "processor.save_pretrained(\"./vit-model-improved\")\n",
    "print(\"‚úÖ Model salvat √Æn './vit-model-improved'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:20:45.544665100Z",
     "start_time": "2025-06-01T13:11:11.933508800Z"
    }
   },
   "id": "83f19c5f5f3a0a5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "‚ûï  \n",
    "    RandomVerticalFlip()\n",
    "\n",
    "    RandomResizedCrop() √Æn loc de CenterCrop\n",
    "\n",
    "    GaussianBlur() (aten»õie la claritate!)\n",
    "\n",
    "üîß  lr_scheduler_type='cosine' sau warmup_steps\n",
    "\n",
    "üß™ Cre»ôte num_train_epochs=10 sau folose»ôte early stopping + evaluare la step\n",
    "\n",
    "MAI MULTE DATE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "876b8b157fd531ae"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logurile au fost salvate √Æn logs.csv\n"
     ]
    }
   ],
   "source": [
    "# === LOGURI ===\n",
    "import pandas as pd\n",
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "log_df.to_csv(\"logs.csv\", index=False)\n",
    "print(\"‚úÖ Logurile au fost salvate √Æn logs.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:07:56.051561600Z",
     "start_time": "2025-06-01T13:07:55.784580100Z"
    }
   },
   "id": "744442ef058257fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d781fa512d03fdd5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
